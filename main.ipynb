{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "from faker import Faker\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data saved to synthetic_payment_descriptions.json\n"
     ]
    }
   ],
   "source": [
    "#Initialize faker\n",
    "fake = Faker(['de_DE', 'sl_SI', 'en_US', 'fr_FR', 'it_IT', 'nl_NL', 'es_ES'])\n",
    "\n",
    "#Contry codes for ibans that we are going to use\n",
    "IBAN_COUNTRIES = {\n",
    "    \"DE\": 22, \"CH\": 21, \"SI\": 19, \"FR\": 27, \"IT\": 27,\n",
    "    \"NL\": 18, \"ES\": 24, \"AT\": 20, \"BE\": 16, \"FI\": 18,\n",
    "    \"LU\": 20, \"MT\": 31, \"PT\": 25, \"SK\": 24, \"CZ\": 24, \"PL\": 28\n",
    "}\n",
    "\n",
    "def generate_iban(country_code):\n",
    "    if country_code in IBAN_COUNTRIES:\n",
    "        random_digits = ''.join(str(random.randint(0, 9)) for _ in range(IBAN_COUNTRIES[country_code] - len(country_code) - 2))\n",
    "        return f\"{country_code}{random.randint(10, 99)}{random_digits}\"\n",
    "    return fake.iban()\n",
    "\n",
    "def generate_entity(entity_type):\n",
    "    if entity_type == \"INVOICE_NUMBER\":\n",
    "        return str(fake.random_int(10000000, 99999999))\n",
    "    elif entity_type == \"REFERENCE_NUMBER\":\n",
    "        return f\"REF-{fake.random_int(1000, 9999)}\"\n",
    "    elif entity_type == \"IBAN\":\n",
    "        return generate_iban(random.choice(list(IBAN_COUNTRIES.keys())))\n",
    "    elif entity_type == \"CONTRACT_NUMBER\":\n",
    "        return f\"CN-{fake.random_int(100000, 999999)}\"\n",
    "    elif entity_type == \"NAME\":\n",
    "        return fake.first_name()\n",
    "    elif entity_type == \"SURNAME\":\n",
    "        return fake.last_name()\n",
    "    return None\n",
    "\n",
    "def generate_descriptions(num_samples=100000):\n",
    "    descriptions = []\n",
    "    # Random sentences where we enter our entities\n",
    "    sentences = [\n",
    "        \"I am paying for my invoice {}. Have a great day!\",\n",
    "        \"Please refer to the reference number {} for further details.\",\n",
    "        \"Here is my IBAN {}. Let me know if you need anything else.\",\n",
    "        \"Contract number {} is being finalized today.\",\n",
    "        \"{} {} will handle the next steps of the project.\",\n",
    "        \"The payment for invoice {} was already made.\",\n",
    "        \"Funds transferred.\",\n",
    "        \"Transaction processed.\"\n",
    "    ]\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        text = random.choice(sentences)\n",
    "        num_entities = random.randint(0, 3)  # Randomly choose 0â€“3 entities\n",
    "        entities = []\n",
    "        occupied_indices = []\n",
    "\n",
    "        for _ in range(num_entities):\n",
    "            entity_type = random.choice([\"INVOICE_NUMBER\", \"REFERENCE_NUMBER\", \"IBAN\", \"CONTRACT_NUMBER\", \"NAME\", \"SURNAME\"])\n",
    "            entity_value = generate_entity(entity_type)\n",
    "\n",
    "            placeholder_index = -1\n",
    "            for i, char in enumerate(text):\n",
    "                if char == \"{\" and i+1 < len(text) and text[i+1] == \"}\":\n",
    "                    placeholder_index = i\n",
    "                    break\n",
    "\n",
    "            if placeholder_index != -1 and entity_value:\n",
    "                text = text[:placeholder_index] + entity_value + text[placeholder_index + 2:]\n",
    "                entities.append({\n",
    "                    \"start\": placeholder_index,\n",
    "                    \"end\": placeholder_index + len(entity_value),\n",
    "                    \"label\": entity_type  # Use \"label\" for spaCy compatibility\n",
    "                })\n",
    "\n",
    "        # Replace any remaining placeholders with filler text\n",
    "        text = text.replace(\"{}\", fake.word())\n",
    "        descriptions.append({\"text\": text, \"entities\": entities})\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "data = generate_descriptions(10000)\n",
    "\n",
    "output_file = \"synthetic_payment_descriptions.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"Generated data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets prepared and saved!\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Loading, Splitting, and DocBin Creation (No changes needed here from the working version)\n",
    "with open(\"synthetic_payment_descriptions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "random.shuffle(full_data)\n",
    "\n",
    "train_size = int(len(full_data) * 0.7)\n",
    "val_size = int(len(full_data) * 0.2)\n",
    "\n",
    "train_data = full_data[:train_size]\n",
    "valid_data = full_data[train_size:train_size + val_size]\n",
    "test_data = full_data[train_size + val_size:]\n",
    "\n",
    "def create_training_data(data, nlp):\n",
    "    doc_bin = DocBin()\n",
    "    for item in data:\n",
    "        text = item[\"text\"]\n",
    "        entities = item[\"entities\"]\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for ent in entities:\n",
    "            start = ent[\"start\"]\n",
    "            end = ent[\"end\"]\n",
    "            label = ent[\"label\"]\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    return doc_bin\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "train_db = create_training_data(train_data, nlp)\n",
    "valid_db = create_training_data(valid_data, nlp)\n",
    "test_db = create_training_data(test_data, nlp)\n",
    "\n",
    "train_db.to_disk(\"train.spacy\")\n",
    "valid_db.to_disk(\"valid.spacy\")\n",
    "test_db.to_disk(\"test.spacy\")\n",
    "\n",
    "print(\"Datasets prepared and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Time: 95.72s - Losses: {'ner': np.float32(1795.9426)} - F Score: 0.9979975971165399\n",
      "Epoch 2/30 - Time: 87.16s - Losses: {'ner': np.float32(873.7676)} - F Score: 0.9979975971165399\n",
      "Epoch 3/30 - Time: 113.76s - Losses: {'ner': np.float32(530.6558)} - F Score: 0.998397435897436\n",
      "Epoch 4/30 - Time: 98.30s - Losses: {'ner': np.float32(358.90723)} - F Score: 0.9979975971165399\n",
      "Epoch 5/30 - Time: 104.73s - Losses: {'ner': np.float32(254.59645)} - F Score: 0.9979975971165399\n",
      "Epoch 6/30 - Time: 105.18s - Losses: {'ner': np.float32(191.53621)} - F Score: 0.9975980784627703\n",
      "Epoch 7/30 - Time: 108.80s - Losses: {'ner': np.float32(158.37415)} - F Score: 0.9979975971165399\n",
      "Epoch 8/30 - Time: 101.23s - Losses: {'ner': np.float32(135.8668)} - F Score: 0.9979975971165399\n",
      "Epoch 9/30 - Time: 114.51s - Losses: {'ner': np.float32(146.70149)} - F Score: 0.998397435897436\n",
      "Epoch 10/30 - Time: 90.48s - Losses: {'ner': np.float32(124.594086)} - F Score: 0.998397435897436\n",
      "Epoch 11/30 - Time: 95.04s - Losses: {'ner': np.float32(107.29403)} - F Score: 0.998397435897436\n",
      "Epoch 12/30 - Time: 79.28s - Losses: {'ner': np.float32(109.957565)} - F Score: 0.9987975951903808\n",
      "Epoch 13/30 - Time: 90.24s - Losses: {'ner': np.float32(107.78887)} - F Score: 0.9987975951903808\n",
      "Epoch 14/30 - Time: 121.37s - Losses: {'ner': np.float32(90.11193)} - F Score: 0.9979975971165399\n",
      "Epoch 15/30 - Time: 127.06s - Losses: {'ner': np.float32(85.517395)} - F Score: 0.9975980784627703\n",
      "Epoch 16/30 - Time: 91.64s - Losses: {'ner': np.float32(93.97439)} - F Score: 0.9979975971165399\n",
      "Epoch 17/30 - Time: 92.73s - Losses: {'ner': np.float32(67.2897)} - F Score: 0.9979975971165399\n",
      "Epoch 18/30 - Time: 108.34s - Losses: {'ner': np.float32(63.99969)} - F Score: 0.9979975971165399\n",
      "Epoch 19/30 - Time: 92.18s - Losses: {'ner': np.float32(77.617195)} - F Score: 0.998397435897436\n",
      "Epoch 20/30 - Time: 83.61s - Losses: {'ner': np.float32(65.828415)} - F Score: 0.9979975971165399\n",
      "Epoch 21/30 - Time: 82.96s - Losses: {'ner': np.float32(80.26489)} - F Score: 0.9979975971165399\n",
      "Epoch 22/30 - Time: 92.01s - Losses: {'ner': np.float32(44.248272)} - F Score: 0.9979975971165399\n",
      "Epoch 23/30 - Time: 97.14s - Losses: {'ner': np.float32(50.4084)} - F Score: 0.9979975971165399\n",
      "Epoch 24/30 - Time: 102.20s - Losses: {'ner': np.float32(70.211426)} - F Score: 0.998397435897436\n",
      "Epoch 25/30 - Time: 102.93s - Losses: {'ner': np.float32(87.45877)} - F Score: 0.9979975971165399\n",
      "Epoch 26/30 - Time: 111.01s - Losses: {'ner': np.float32(60.31969)} - F Score: 0.998397435897436\n",
      "Epoch 27/30 - Time: 105.83s - Losses: {'ner': np.float32(49.683544)} - F Score: 0.9979975971165399\n",
      "Epoch 28/30 - Time: 118.29s - Losses: {'ner': np.float32(57.663136)} - F Score: 0.9979975971165399\n",
      "Epoch 29/30 - Time: 142.99s - Losses: {'ner': np.float32(52.482742)} - F Score: 0.9991980753809142\n",
      "Epoch 30/30 - Time: 139.63s - Losses: {'ner': np.float32(47.24024)} - F Score: 0.9975980784627703\n",
      "Model trained and saved to ./model\n"
     ]
    }
   ],
   "source": [
    "# Training the NER model\n",
    "nlp = spacy.blank(\"en\")\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "labels = [\"INVOICE_NUMBER\", \"REFERENCE_NUMBER\", \"IBAN\", \"CONTRACT_NUMBER\", \"NAME\", \"SURNAME\"]\n",
    "for label in labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "train_db = DocBin().from_disk(\"train.spacy\")\n",
    "valid_db = DocBin().from_disk(\"valid.spacy\")\n",
    "\n",
    "train_examples = []\n",
    "for doc in train_db.get_docs(nlp.vocab):\n",
    "    if doc.ents:\n",
    "        train_examples.append(Example.from_dict(doc, {\"entities\": doc.ents}))\n",
    "\n",
    "valid_examples = []\n",
    "for doc in valid_db.get_docs(nlp.vocab):\n",
    "    if doc.ents:\n",
    "        valid_examples.append(Example.from_dict(doc, {\"entities\": doc.ents}))\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.initialize()\n",
    "    n_iter = 30 #Number of iterations\n",
    "    for i in range(n_iter):\n",
    "        start_time = time.time()\n",
    "        losses = {}\n",
    "        batches = spacy.util.minibatch(train_examples, size=2)\n",
    "        for batch in batches:\n",
    "            try:\n",
    "                nlp.update(batch, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                for example in batch:\n",
    "                    print(example.text)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        with nlp.disable_pipes(*other_pipes):\n",
    "            scores = nlp.evaluate(valid_examples)\n",
    "\n",
    "        print(f\"Epoch {i + 1}/{n_iter} - Time: {epoch_time:.2f}s - Losses: {losses} - F Score: {scores['ents_f']}\")\n",
    "\n",
    "nlp.to_disk(\"./model\")\n",
    "print(\"Model trained and saved to ./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "Overall F-score: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "\n",
      "Per-entity metrics:\n",
      "\n",
      "CONTRACT_NUMBER:\n",
      "  F-score: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "\n",
      "NAME:\n",
      "  F-score: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "\n",
      "REFERENCE_NUMBER:\n",
      "  F-score: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "\n",
      "INVOICE_NUMBER:\n",
      "  F-score: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "\n",
      "IBAN:\n",
      "  F-score: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "\n",
      "SURNAME:\n",
      "  F-score: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "Payment SURNAME\n",
      "12345678 INVOICE_NUMBER\n",
      "John Doe NAME\n",
      "REF-9876 REFERENCE_NUMBER\n",
      ": NAME\n",
      "DE12345678901234567890 IBAN\n",
      "Happy NAME\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "nlp_trained = spacy.load(\"./model\")\n",
    "\n",
    "# Load the test data\n",
    "test_db = DocBin().from_disk(\"test.spacy\")\n",
    "examples = []\n",
    "for doc in test_db.get_docs(nlp_trained.vocab):\n",
    "    if doc.ents:\n",
    "        examples.append(Example.from_dict(doc, {\"entities\": doc.ents}))\n",
    "\n",
    "# Evaluate the model\n",
    "scorer = Scorer()\n",
    "scores = scorer.score(examples)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"Evaluation results:\")\n",
    "print(f\"Overall F-score: {scores['ents_f']:.4f}\")\n",
    "print(f\"Precision: {scores['ents_p']:.4f}\")\n",
    "print(f\"Recall: {scores['ents_r']:.4f}\")\n",
    "\n",
    "# Print per-entity metrics\n",
    "print(\"\\nPer-entity metrics:\")\n",
    "for metric in [\"ents_per_type\"]:\n",
    "    for entity_type, entity_scores in scores[metric].items():\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        print(f\"  F-score: {entity_scores['f']:.4f}\")\n",
    "        print(f\"  Precision: {entity_scores['p']:.4f}\")\n",
    "        print(f\"  Recall: {entity_scores['r']:.4f}\")\n",
    "\n",
    "# Example Usage\n",
    "text = \"Payment for invoice 12345678 to John Doe with reference REF-9876. IBAN: DE12345678901234567890\"\n",
    "doc = nlp_trained(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "text = \"Happy new year! I am paying rent.\"\n",
    "doc = nlp_trained(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
